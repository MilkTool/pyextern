/* This file is generated, do not edit! */
package tensorflow.python.ops.variables;
@:pythonImport("tensorflow.python.ops.variables", "VariableV1") extern class VariableV1 {
	/**
		Information on how to save this Variable as a slice.
		
		Provides internal support for saving variables as slices of a larger
		variable.  This API is not public and is subject to change.
		
		Available properties:
		
		* full_name
		* full_shape
		* var_offset
		* var_shape
	**/
	static public function SaveSliceInfo(?full_name:Dynamic, ?full_shape:Dynamic, ?var_offset:Dynamic, ?var_shape:Dynamic, ?save_slice_info_def:Dynamic, ?import_scope:Dynamic):Dynamic;
	/**
		Register overloads for all operators.
	**/
	static public function _OverloadAllOperators():Dynamic;
	/**
		Defer an operator overload to `ops.Tensor`.
		
		We pull the operator out of ops.Tensor dynamically to avoid ordering issues.
		
		Args:
		  operator: string. The operator name.
	**/
	static public function _OverloadOperator(_operator:Dynamic):Dynamic;
	/**
		Utility function for converting a Variable to a Tensor.
	**/
	static public function _TensorConversionFunction(v:Dynamic, ?dtype:Dynamic, ?name:Dynamic, ?as_ref:Dynamic):Dynamic;
	/**
		Computes the absolute value of a tensor.
		
		Given a tensor `x` of complex numbers, this operation returns a tensor of type
		`float32` or `float64` that is the absolute value of each element in `x`. All
		elements in `x` must be complex numbers of the form \\(a + bj\\). The
		absolute value is computed as \\( \sqrt{a^2 + b^2}\\).  For example:
		```python
		x = tf.constant([[-2.25 + 4.75j], [-3.25 + 5.75j]])
		tf.abs(x)  # [5.25594902, 6.60492229]
		```
		
		Args:
		  x: A `Tensor` or `SparseTensor` of type `float16`, `float32`, `float64`,
		    `int32`, `int64`, `complex64` or `complex128`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor` or `SparseTensor` the same size and type as `x` with absolute
		    values.
		  Note, for `complex64` or `complex128` input, the returned `Tensor` will be
		    of type `float32` or `float64`, respectively.
	**/
	static public function __abs__(a:Dynamic, ?args:python.VarArgs<Dynamic>):Dynamic;
	/**
		Returns x + y element-wise.
		
		*NOTE*: `math.add` supports broadcasting. `AddN` does not. More about broadcasting
		[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
		
		Args:
		  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`, `string`.
		  y: A `Tensor`. Must have the same type as `x`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor`. Has the same type as `x`.
	**/
	static public function __add__(a:Dynamic, ?args:python.VarArgs<Dynamic>):Dynamic;
	/**
		Returns the truth value of x AND y element-wise.
		
		*NOTE*: `math.logical_and` supports broadcasting. More about broadcasting
		[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
		
		Args:
		  x: A `Tensor` of type `bool`.
		  y: A `Tensor` of type `bool`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor` of type `bool`.
	**/
	static public function __and__(a:Dynamic, ?args:python.VarArgs<Dynamic>):Dynamic;
	static public var __array_priority__ : Dynamic;
	/**
		Metaclass to allow construction of tf.Variable to be overridden.
	**/
	public function __class__(args:haxe.extern.Rest<Dynamic>):Dynamic;
	/**
		Implement delattr(self, name).
	**/
	public function __delattr__(name:Dynamic):Dynamic;
	static public var __dict__ : Dynamic;
	/**
		__dir__() -> list
		default dir() implementation
	**/
	public function __dir__(args:haxe.extern.Rest<Dynamic>):Dynamic;
	/**
		Divide two values using Python 2 semantics. Used for Tensor.__div__.
		
		Args:
		  x: `Tensor` numerator of real numeric type.
		  y: `Tensor` denominator of real numeric type.
		  name: A name for the operation (optional).
		Returns:
		  `x / y` returns the quotient of x and y.
	**/
	static public function __div__(a:Dynamic, ?args:python.VarArgs<Dynamic>):Dynamic;
	static public var __doc__ : Dynamic;
	/**
		Return self==value.
	**/
	public function __eq__(value:Dynamic):Dynamic;
	/**
		Divides `x / y` elementwise, rounding toward the most negative integer.
		
		The same as `tf.div(x,y)` for integers, but uses `tf.floor(tf.div(x,y))` for
		floating point arguments so that the result is always an integer (though
		possibly an integer represented as floating point).  This op is generated by
		`x // y` floor division in Python 3 and in Python 2.7 with
		`from __future__ import division`.
		
		`x` and `y` must have the same type, and the result will have the same type
		as well.
		
		Args:
		  x: `Tensor` numerator of real numeric type.
		  y: `Tensor` denominator of real numeric type.
		  name: A name for the operation (optional).
		
		Returns:
		  `x / y` rounded down.
		
		Raises:
		  TypeError: If the inputs are complex.
	**/
	static public function __floordiv__(a:Dynamic, ?args:python.VarArgs<Dynamic>):Dynamic;
	/**
		default object formatter
	**/
	public function __format__(args:haxe.extern.Rest<Dynamic>):Dynamic;
	/**
		Returns the truth value of (x >= y) element-wise.
		
		*NOTE*: `math.greater_equal` supports broadcasting. More about broadcasting
		[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
		
		Args:
		  x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.
		  y: A `Tensor`. Must have the same type as `x`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor` of type `bool`.
	**/
	static public function __ge__(a:Dynamic, ?args:python.VarArgs<Dynamic>):Dynamic;
	/**
		Return getattr(self, name).
	**/
	public function __getattribute__(name:Dynamic):Dynamic;
	/**
		Creates a slice helper object given a variable.
		
		This allows creating a sub-tensor from part of the current contents
		of a variable. See `tf.Tensor.__getitem__` for detailed examples
		of slicing.
		
		This function in addition also allows assignment to a sliced range.
		This is similar to `__setitem__` functionality in Python. However,
		the syntax is different so that the user can capture the assignment
		operation for grouping or passing to `sess.run()`.
		For example,
		
		```python
		import tensorflow as tf
		A = tf.Variable([[1,2,3], [4,5,6], [7,8,9]], dtype=tf.float32)
		with tf.Session() as sess:
		  sess.run(tf.global_variables_initializer())
		  print(sess.run(A[:2, :2]))  # => [[1,2], [4,5]]
		
		  op = A[:2,:2].assign(22. * tf.ones((2, 2)))
		  print(sess.run(op))  # => [[22, 22, 3], [22, 22, 6], [7,8,9]]
		```
		
		Note that assignments currently do not support NumPy broadcasting
		semantics.
		
		Args:
		  var: An `ops.Variable` object.
		  slice_spec: The arguments to `Tensor.__getitem__`.
		
		Returns:
		  The appropriate slice of "tensor", based on "slice_spec".
		  As an operator. The operator also has a `assign()` method
		  that can be used to generate an assignment operator.
		
		Raises:
		  ValueError: If a slice range is negative size.
		  TypeError: If the slice indices aren't int, slice, or Ellipsis.
	**/
	static public function __getitem__(_var:Dynamic, slice_spec:Dynamic):Dynamic;
	/**
		Returns the truth value of (x > y) element-wise.
		
		*NOTE*: `math.greater` supports broadcasting. More about broadcasting
		[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
		
		Args:
		  x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.
		  y: A `Tensor`. Must have the same type as `x`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor` of type `bool`.
	**/
	static public function __gt__(a:Dynamic, ?args:python.VarArgs<Dynamic>):Dynamic;
	/**
		Return hash(self).
	**/
	public function __hash__():Dynamic;
	public function __iadd__(other:Dynamic):Dynamic;
	public function __idiv__(other:Dynamic):Dynamic;
	public function __imul__(other:Dynamic):Dynamic;
	/**
		Creates a new variable with value `initial_value`.
		
		The new variable is added to the graph collections listed in `collections`,
		which defaults to `[GraphKeys.GLOBAL_VARIABLES]`.
		
		If `trainable` is `True` the variable is also added to the graph collection
		`GraphKeys.TRAINABLE_VARIABLES`.
		
		This constructor creates both a `variable` Op and an `assign` Op to set the
		variable to its initial value.
		
		Args:
		  initial_value: A `Tensor`, or Python object convertible to a `Tensor`,
		    which is the initial value for the Variable. The initial value must have
		    a shape specified unless `validate_shape` is set to False. Can also be a
		    callable with no argument that returns the initial value when called. In
		    that case, `dtype` must be specified. (Note that initializer functions
		    from init_ops.py must first be bound to a shape before being used here.)
		  trainable: If `True`, the default, also adds the variable to the graph
		    collection `GraphKeys.TRAINABLE_VARIABLES`. This collection is used as
		    the default list of variables to use by the `Optimizer` classes.
		  collections: List of graph collections keys. The new variable is added to
		    these collections. Defaults to `[GraphKeys.GLOBAL_VARIABLES]`.
		  validate_shape: If `False`, allows the variable to be initialized with a
		    value of unknown shape. If `True`, the default, the shape of
		    `initial_value` must be known.
		  caching_device: Optional device string describing where the Variable
		    should be cached for reading.  Defaults to the Variable's device.
		    If not `None`, caches on another device.  Typical use is to cache
		    on the device where the Ops using the Variable reside, to deduplicate
		    copying through `Switch` and other conditional statements.
		  name: Optional name for the variable. Defaults to `'Variable'` and gets
		    uniquified automatically.
		  variable_def: `VariableDef` protocol buffer. If not `None`, recreates
		    the Variable object with its contents, referencing the variable's nodes
		    in the graph, which must already exist. The graph is not changed.
		    `variable_def` and the other arguments are mutually exclusive.
		  dtype: If set, initial_value will be converted to the given type.
		    If `None`, either the datatype will be kept (if `initial_value` is
		    a Tensor), or `convert_to_tensor` will decide.
		  expected_shape: A TensorShape. If set, initial_value is expected
		    to have this shape.
		  import_scope: Optional `string`. Name scope to add to the
		    `Variable.` Only used when initializing from protocol buffer.
		  constraint: An optional projection function to be applied to the variable
		    after being updated by an `Optimizer` (e.g. used to implement norm
		    constraints or value constraints for layer weights). The function must
		    take as input the unprojected Tensor representing the value of the
		    variable and return the Tensor for the projected value
		    (which must have the same shape). Constraints are not safe to
		    use when doing asynchronous distributed training.
		  use_resource: whether to use resource variables.
		  synchronization: unused
		  aggregation: unused
		
		Raises:
		  ValueError: If both `variable_def` and initial_value are specified.
		  ValueError: If the initial value is not specified, or does not have a
		    shape and `validate_shape` is `True`.
		  RuntimeError: If eager execution is enabled.
	**/
	@:native("__init__")
	public function ___init__(?initial_value:Dynamic, ?trainable:Dynamic, ?collections:Dynamic, ?validate_shape:Dynamic, ?caching_device:Dynamic, ?name:Dynamic, ?variable_def:Dynamic, ?dtype:Dynamic, ?expected_shape:Dynamic, ?import_scope:Dynamic, ?constraint:Dynamic, ?use_resource:Dynamic, ?synchronization:Dynamic, ?aggregation:Dynamic):Dynamic;
	/**
		Creates a new variable with value `initial_value`.
		
		The new variable is added to the graph collections listed in `collections`,
		which defaults to `[GraphKeys.GLOBAL_VARIABLES]`.
		
		If `trainable` is `True` the variable is also added to the graph collection
		`GraphKeys.TRAINABLE_VARIABLES`.
		
		This constructor creates both a `variable` Op and an `assign` Op to set the
		variable to its initial value.
		
		Args:
		  initial_value: A `Tensor`, or Python object convertible to a `Tensor`,
		    which is the initial value for the Variable. The initial value must have
		    a shape specified unless `validate_shape` is set to False. Can also be a
		    callable with no argument that returns the initial value when called. In
		    that case, `dtype` must be specified. (Note that initializer functions
		    from init_ops.py must first be bound to a shape before being used here.)
		  trainable: If `True`, the default, also adds the variable to the graph
		    collection `GraphKeys.TRAINABLE_VARIABLES`. This collection is used as
		    the default list of variables to use by the `Optimizer` classes.
		  collections: List of graph collections keys. The new variable is added to
		    these collections. Defaults to `[GraphKeys.GLOBAL_VARIABLES]`.
		  validate_shape: If `False`, allows the variable to be initialized with a
		    value of unknown shape. If `True`, the default, the shape of
		    `initial_value` must be known.
		  caching_device: Optional device string describing where the Variable
		    should be cached for reading.  Defaults to the Variable's device.
		    If not `None`, caches on another device.  Typical use is to cache
		    on the device where the Ops using the Variable reside, to deduplicate
		    copying through `Switch` and other conditional statements.
		  name: Optional name for the variable. Defaults to `'Variable'` and gets
		    uniquified automatically.
		  variable_def: `VariableDef` protocol buffer. If not `None`, recreates
		    the Variable object with its contents, referencing the variable's nodes
		    in the graph, which must already exist. The graph is not changed.
		    `variable_def` and the other arguments are mutually exclusive.
		  dtype: If set, initial_value will be converted to the given type.
		    If `None`, either the datatype will be kept (if `initial_value` is
		    a Tensor), or `convert_to_tensor` will decide.
		  expected_shape: A TensorShape. If set, initial_value is expected
		    to have this shape.
		  import_scope: Optional `string`. Name scope to add to the
		    `Variable.` Only used when initializing from protocol buffer.
		  constraint: An optional projection function to be applied to the variable
		    after being updated by an `Optimizer` (e.g. used to implement norm
		    constraints or value constraints for layer weights). The function must
		    take as input the unprojected Tensor representing the value of the
		    variable and return the Tensor for the projected value
		    (which must have the same shape). Constraints are not safe to
		    use when doing asynchronous distributed training.
		  use_resource: whether to use resource variables.
		  synchronization: unused
		  aggregation: unused
		
		Raises:
		  ValueError: If both `variable_def` and initial_value are specified.
		  ValueError: If the initial value is not specified, or does not have a
		    shape and `validate_shape` is `True`.
		  RuntimeError: If eager execution is enabled.
	**/
	public function new(?initial_value:Dynamic, ?trainable:Dynamic, ?collections:Dynamic, ?validate_shape:Dynamic, ?caching_device:Dynamic, ?name:Dynamic, ?variable_def:Dynamic, ?dtype:Dynamic, ?expected_shape:Dynamic, ?import_scope:Dynamic, ?constraint:Dynamic, ?use_resource:Dynamic, ?synchronization:Dynamic, ?aggregation:Dynamic):Void;
	/**
		This method is called when a class is subclassed.
		
		The default implementation does nothing. It may be
		overridden to extend subclasses.
	**/
	public function __init_subclass__(args:haxe.extern.Rest<Dynamic>):Dynamic;
	/**
		Returns the truth value of NOT x element-wise.
		
		Args:
		  x: A `Tensor` of type `bool`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor` of type `bool`.
	**/
	static public function __invert__(a:Dynamic, ?args:python.VarArgs<Dynamic>):Dynamic;
	public function __ipow__(other:Dynamic):Dynamic;
	public function __irealdiv__(other:Dynamic):Dynamic;
	public function __isub__(other:Dynamic):Dynamic;
	public function __itruediv__(other:Dynamic):Dynamic;
	/**
		Returns the truth value of (x <= y) element-wise.
		
		*NOTE*: `math.less_equal` supports broadcasting. More about broadcasting
		[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
		
		Args:
		  x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.
		  y: A `Tensor`. Must have the same type as `x`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor` of type `bool`.
	**/
	static public function __le__(a:Dynamic, ?args:python.VarArgs<Dynamic>):Dynamic;
	/**
		Returns the truth value of (x < y) element-wise.
		
		*NOTE*: `math.less` supports broadcasting. More about broadcasting
		[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
		
		Args:
		  x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.
		  y: A `Tensor`. Must have the same type as `x`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor` of type `bool`.
	**/
	static public function __lt__(a:Dynamic, ?args:python.VarArgs<Dynamic>):Dynamic;
	/**
		Multiplies matrix `a` by matrix `b`, producing `a` * `b`.
		
		The inputs must, following any transpositions, be tensors of rank >= 2
		where the inner 2 dimensions specify valid matrix multiplication arguments,
		and any further outer dimensions match.
		
		Both matrices must be of the same type. The supported types are:
		`float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.
		
		Either matrix can be transposed or adjointed (conjugated and transposed) on
		the fly by setting one of the corresponding flag to `True`. These are `False`
		by default.
		
		If one or both of the matrices contain a lot of zeros, a more efficient
		multiplication algorithm can be used by setting the corresponding
		`a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.
		This optimization is only available for plain matrices (rank-2 tensors) with
		datatypes `bfloat16` or `float32`.
		
		For example:
		
		```python
		# 2-D tensor `a`
		# [[1, 2, 3],
		#  [4, 5, 6]]
		a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])
		
		# 2-D tensor `b`
		# [[ 7,  8],
		#  [ 9, 10],
		#  [11, 12]]
		b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])
		
		# `a` * `b`
		# [[ 58,  64],
		#  [139, 154]]
		c = tf.matmul(a, b)
		
		
		# 3-D tensor `a`
		# [[[ 1,  2,  3],
		#   [ 4,  5,  6]],
		#  [[ 7,  8,  9],
		#   [10, 11, 12]]]
		a = tf.constant(np.arange(1, 13, dtype=np.int32),
		                shape=[2, 2, 3])
		
		# 3-D tensor `b`
		# [[[13, 14],
		#   [15, 16],
		#   [17, 18]],
		#  [[19, 20],
		#   [21, 22],
		#   [23, 24]]]
		b = tf.constant(np.arange(13, 25, dtype=np.int32),
		                shape=[2, 3, 2])
		
		# `a` * `b`
		# [[[ 94, 100],
		#   [229, 244]],
		#  [[508, 532],
		#   [697, 730]]]
		c = tf.matmul(a, b)
		
		# Since python >= 3.5 the @ operator is supported (see PEP 465).
		# In TensorFlow, it simply calls the `tf.matmul()` function, so the
		# following lines are equivalent:
		d = a @ b @ [[10.], [11.]]
		d = tf.matmul(tf.matmul(a, b), [[10.], [11.]])
		```
		
		Args:
		  a: `Tensor` of type `float16`, `float32`, `float64`, `int32`, `complex64`,
		    `complex128` and rank > 1.
		  b: `Tensor` with same type and rank as `a`.
		  transpose_a: If `True`, `a` is transposed before multiplication.
		  transpose_b: If `True`, `b` is transposed before multiplication.
		  adjoint_a: If `True`, `a` is conjugated and transposed before
		    multiplication.
		  adjoint_b: If `True`, `b` is conjugated and transposed before
		    multiplication.
		  a_is_sparse: If `True`, `a` is treated as a sparse matrix.
		  b_is_sparse: If `True`, `b` is treated as a sparse matrix.
		  name: Name for the operation (optional).
		
		Returns:
		  A `Tensor` of the same type as `a` and `b` where each inner-most matrix is
		  the product of the corresponding matrices in `a` and `b`, e.g. if all
		  transpose or adjoint attributes are `False`:
		
		  `output`[..., i, j] = sum_k (`a`[..., i, k] * `b`[..., k, j]),
		  for all indices i, j.
		
		  Note: This is matrix product, not element-wise product.
		
		
		Raises:
		  ValueError: If transpose_a and adjoint_a, or transpose_b and adjoint_b
		    are both set to True.
	**/
	static public function __matmul__(a:Dynamic, ?args:python.VarArgs<Dynamic>):Dynamic;
	/**
		Returns element-wise remainder of division. When `x < 0` xor `y < 0` is
		
		true, this follows Python semantics in that the result here is consistent
		with a flooring divide. E.g. `floor(x / y) * y + mod(x, y) = x`.
		
		*NOTE*: `FloorMod` supports broadcasting. More about broadcasting
		[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
		
		Args:
		  x: A `Tensor`. Must be one of the following types: `int32`, `int64`, `bfloat16`, `half`, `float32`, `float64`.
		  y: A `Tensor`. Must have the same type as `x`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor`. Has the same type as `x`.
	**/
	static public function __mod__(a:Dynamic, ?args:python.VarArgs<Dynamic>):Dynamic;
	static public var __module__ : Dynamic;
	/**
		Dispatches cwise mul for "Dense*Dense" and "Dense*Sparse".
	**/
	static public function __mul__(a:Dynamic, ?args:python.VarArgs<Dynamic>):Dynamic;
	/**
		Return self!=value.
	**/
	public function __ne__(value:Dynamic):Dynamic;
	/**
		Computes numerical negative value element-wise.
		
		I.e., \\(y = -x\\).
		
		Args:
		  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int32`, `int64`, `complex64`, `complex128`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor`. Has the same type as `x`.
	**/
	static public function __neg__(a:Dynamic, ?args:python.VarArgs<Dynamic>):Dynamic;
	/**
		Create and return a new object.  See help(type) for accurate signature.
	**/
	static public function __new__(?args:python.VarArgs<Dynamic>, ?kwargs:python.KwArgs<Dynamic>):Dynamic;
	/**
		Returns the truth value of x OR y element-wise.
		
		*NOTE*: `math.logical_or` supports broadcasting. More about broadcasting
		[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
		
		Args:
		  x: A `Tensor` of type `bool`.
		  y: A `Tensor` of type `bool`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor` of type `bool`.
	**/
	static public function __or__(a:Dynamic, ?args:python.VarArgs<Dynamic>):Dynamic;
	/**
		Computes the power of one value to another.
		
		Given a tensor `x` and a tensor `y`, this operation computes \\(x^y\\) for
		corresponding elements in `x` and `y`. For example:
		
		```python
		x = tf.constant([[2, 2], [3, 3]])
		y = tf.constant([[8, 16], [2, 3]])
		tf.pow(x, y)  # [[256, 65536], [9, 27]]
		```
		
		Args:
		  x: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,
		   `complex64`, or `complex128`.
		  y: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,
		   `complex64`, or `complex128`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor`.
	**/
	static public function __pow__(a:Dynamic, ?args:python.VarArgs<Dynamic>):Dynamic;
	/**
		Returns x + y element-wise.
		
		*NOTE*: `math.add` supports broadcasting. `AddN` does not. More about broadcasting
		[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
		
		Args:
		  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`, `string`.
		  y: A `Tensor`. Must have the same type as `x`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor`. Has the same type as `x`.
	**/
	static public function __radd__(a:Dynamic, ?args:python.VarArgs<Dynamic>):Dynamic;
	/**
		Returns the truth value of x AND y element-wise.
		
		*NOTE*: `math.logical_and` supports broadcasting. More about broadcasting
		[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
		
		Args:
		  x: A `Tensor` of type `bool`.
		  y: A `Tensor` of type `bool`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor` of type `bool`.
	**/
	static public function __rand__(a:Dynamic, ?args:python.VarArgs<Dynamic>):Dynamic;
	/**
		Divide two values using Python 2 semantics. Used for Tensor.__div__.
		
		Args:
		  x: `Tensor` numerator of real numeric type.
		  y: `Tensor` denominator of real numeric type.
		  name: A name for the operation (optional).
		Returns:
		  `x / y` returns the quotient of x and y.
	**/
	static public function __rdiv__(a:Dynamic, ?args:python.VarArgs<Dynamic>):Dynamic;
	/**
		helper for pickle
	**/
	public function __reduce__(args:haxe.extern.Rest<Dynamic>):Dynamic;
	/**
		helper for pickle
	**/
	public function __reduce_ex__(args:haxe.extern.Rest<Dynamic>):Dynamic;
	/**
		Return repr(self).
	**/
	public function __repr__():Dynamic;
	/**
		Divides `x / y` elementwise, rounding toward the most negative integer.
		
		The same as `tf.div(x,y)` for integers, but uses `tf.floor(tf.div(x,y))` for
		floating point arguments so that the result is always an integer (though
		possibly an integer represented as floating point).  This op is generated by
		`x // y` floor division in Python 3 and in Python 2.7 with
		`from __future__ import division`.
		
		`x` and `y` must have the same type, and the result will have the same type
		as well.
		
		Args:
		  x: `Tensor` numerator of real numeric type.
		  y: `Tensor` denominator of real numeric type.
		  name: A name for the operation (optional).
		
		Returns:
		  `x / y` rounded down.
		
		Raises:
		  TypeError: If the inputs are complex.
	**/
	static public function __rfloordiv__(a:Dynamic, ?args:python.VarArgs<Dynamic>):Dynamic;
	/**
		Multiplies matrix `a` by matrix `b`, producing `a` * `b`.
		
		The inputs must, following any transpositions, be tensors of rank >= 2
		where the inner 2 dimensions specify valid matrix multiplication arguments,
		and any further outer dimensions match.
		
		Both matrices must be of the same type. The supported types are:
		`float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.
		
		Either matrix can be transposed or adjointed (conjugated and transposed) on
		the fly by setting one of the corresponding flag to `True`. These are `False`
		by default.
		
		If one or both of the matrices contain a lot of zeros, a more efficient
		multiplication algorithm can be used by setting the corresponding
		`a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.
		This optimization is only available for plain matrices (rank-2 tensors) with
		datatypes `bfloat16` or `float32`.
		
		For example:
		
		```python
		# 2-D tensor `a`
		# [[1, 2, 3],
		#  [4, 5, 6]]
		a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])
		
		# 2-D tensor `b`
		# [[ 7,  8],
		#  [ 9, 10],
		#  [11, 12]]
		b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])
		
		# `a` * `b`
		# [[ 58,  64],
		#  [139, 154]]
		c = tf.matmul(a, b)
		
		
		# 3-D tensor `a`
		# [[[ 1,  2,  3],
		#   [ 4,  5,  6]],
		#  [[ 7,  8,  9],
		#   [10, 11, 12]]]
		a = tf.constant(np.arange(1, 13, dtype=np.int32),
		                shape=[2, 2, 3])
		
		# 3-D tensor `b`
		# [[[13, 14],
		#   [15, 16],
		#   [17, 18]],
		#  [[19, 20],
		#   [21, 22],
		#   [23, 24]]]
		b = tf.constant(np.arange(13, 25, dtype=np.int32),
		                shape=[2, 3, 2])
		
		# `a` * `b`
		# [[[ 94, 100],
		#   [229, 244]],
		#  [[508, 532],
		#   [697, 730]]]
		c = tf.matmul(a, b)
		
		# Since python >= 3.5 the @ operator is supported (see PEP 465).
		# In TensorFlow, it simply calls the `tf.matmul()` function, so the
		# following lines are equivalent:
		d = a @ b @ [[10.], [11.]]
		d = tf.matmul(tf.matmul(a, b), [[10.], [11.]])
		```
		
		Args:
		  a: `Tensor` of type `float16`, `float32`, `float64`, `int32`, `complex64`,
		    `complex128` and rank > 1.
		  b: `Tensor` with same type and rank as `a`.
		  transpose_a: If `True`, `a` is transposed before multiplication.
		  transpose_b: If `True`, `b` is transposed before multiplication.
		  adjoint_a: If `True`, `a` is conjugated and transposed before
		    multiplication.
		  adjoint_b: If `True`, `b` is conjugated and transposed before
		    multiplication.
		  a_is_sparse: If `True`, `a` is treated as a sparse matrix.
		  b_is_sparse: If `True`, `b` is treated as a sparse matrix.
		  name: Name for the operation (optional).
		
		Returns:
		  A `Tensor` of the same type as `a` and `b` where each inner-most matrix is
		  the product of the corresponding matrices in `a` and `b`, e.g. if all
		  transpose or adjoint attributes are `False`:
		
		  `output`[..., i, j] = sum_k (`a`[..., i, k] * `b`[..., k, j]),
		  for all indices i, j.
		
		  Note: This is matrix product, not element-wise product.
		
		
		Raises:
		  ValueError: If transpose_a and adjoint_a, or transpose_b and adjoint_b
		    are both set to True.
	**/
	static public function __rmatmul__(a:Dynamic, ?args:python.VarArgs<Dynamic>):Dynamic;
	/**
		Returns element-wise remainder of division. When `x < 0` xor `y < 0` is
		
		true, this follows Python semantics in that the result here is consistent
		with a flooring divide. E.g. `floor(x / y) * y + mod(x, y) = x`.
		
		*NOTE*: `FloorMod` supports broadcasting. More about broadcasting
		[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
		
		Args:
		  x: A `Tensor`. Must be one of the following types: `int32`, `int64`, `bfloat16`, `half`, `float32`, `float64`.
		  y: A `Tensor`. Must have the same type as `x`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor`. Has the same type as `x`.
	**/
	static public function __rmod__(a:Dynamic, ?args:python.VarArgs<Dynamic>):Dynamic;
	/**
		Dispatches cwise mul for "Dense*Dense" and "Dense*Sparse".
	**/
	static public function __rmul__(a:Dynamic, ?args:python.VarArgs<Dynamic>):Dynamic;
	/**
		Returns the truth value of x OR y element-wise.
		
		*NOTE*: `math.logical_or` supports broadcasting. More about broadcasting
		[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
		
		Args:
		  x: A `Tensor` of type `bool`.
		  y: A `Tensor` of type `bool`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor` of type `bool`.
	**/
	static public function __ror__(a:Dynamic, ?args:python.VarArgs<Dynamic>):Dynamic;
	/**
		Computes the power of one value to another.
		
		Given a tensor `x` and a tensor `y`, this operation computes \\(x^y\\) for
		corresponding elements in `x` and `y`. For example:
		
		```python
		x = tf.constant([[2, 2], [3, 3]])
		y = tf.constant([[8, 16], [2, 3]])
		tf.pow(x, y)  # [[256, 65536], [9, 27]]
		```
		
		Args:
		  x: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,
		   `complex64`, or `complex128`.
		  y: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,
		   `complex64`, or `complex128`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor`.
	**/
	static public function __rpow__(a:Dynamic, ?args:python.VarArgs<Dynamic>):Dynamic;
	/**
		Returns x - y element-wise.
		
		*NOTE*: `Subtract` supports broadcasting. More about broadcasting
		[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
		
		Args:
		  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`.
		  y: A `Tensor`. Must have the same type as `x`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor`. Has the same type as `x`.
	**/
	static public function __rsub__(a:Dynamic, ?args:python.VarArgs<Dynamic>):Dynamic;
	static public function __rtruediv__(a:Dynamic, ?args:python.VarArgs<Dynamic>):Dynamic;
	/**
		x ^ y = (x | y) & ~(x & y).
	**/
	static public function __rxor__(a:Dynamic, ?args:python.VarArgs<Dynamic>):Dynamic;
	/**
		Implement setattr(self, name, value).
	**/
	public function __setattr__(name:Dynamic, value:Dynamic):Dynamic;
	/**
		__sizeof__() -> int
		size of object in memory, in bytes
	**/
	public function __sizeof__(args:haxe.extern.Rest<Dynamic>):Dynamic;
	/**
		Return str(self).
	**/
	public function __str__():Dynamic;
	/**
		Returns x - y element-wise.
		
		*NOTE*: `Subtract` supports broadcasting. More about broadcasting
		[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
		
		Args:
		  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`.
		  y: A `Tensor`. Must have the same type as `x`.
		  name: A name for the operation (optional).
		
		Returns:
		  A `Tensor`. Has the same type as `x`.
	**/
	static public function __sub__(a:Dynamic, ?args:python.VarArgs<Dynamic>):Dynamic;
	/**
		Abstract classes can override this to customize issubclass().
		
		This is invoked early on by abc.ABCMeta.__subclasscheck__().
		It should return True, False or NotImplemented.  If it returns
		NotImplemented, the normal algorithm is used.  Otherwise, it
		overrides the normal algorithm (and the outcome is cached).
	**/
	public function __subclasshook__(args:haxe.extern.Rest<Dynamic>):Dynamic;
	static public function __truediv__(a:Dynamic, ?args:python.VarArgs<Dynamic>):Dynamic;
	/**
		list of weak references to the object (if defined)
	**/
	public var __weakref__ : Dynamic;
	/**
		x ^ y = (x | y) & ~(x & y).
	**/
	static public function __xor__(a:Dynamic, ?args:python.VarArgs<Dynamic>):Dynamic;
	/**
		Restore-on-create for a variable be saved with this `Checkpointable`.
		
		If the user has requested that this object or another `Checkpointable` which
		depends on this object be restored from a checkpoint (deferred loading
		before variable object creation), `initializer` may be ignored and the value
		from the checkpoint used instead.
		
		Args:
		  name: A name for the variable. Must be unique within this object.
		  shape: The shape of the variable.
		  dtype: The data type of the variable.
		  initializer: The initializer to use. Ignored if there is a deferred
		    restoration left over from a call to
		    `_restore_from_checkpoint_position`.
		  getter: The getter to wrap which actually fetches the variable.
		  overwrite: If True, disables unique name and type checks.
		  **kwargs_for_getter: Passed to the getter.
		
		Returns:
		  The new variable object.
		
		Raises:
		  ValueError: If the variable name is not unique.
	**/
	public function _add_variable_with_custom_getter(name:Dynamic, ?shape:Dynamic, ?dtype:Dynamic, ?initializer:Dynamic, ?getter:Dynamic, ?overwrite:Dynamic, ?kwargs_for_getter:python.KwArgs<Dynamic>):Dynamic;
	/**
		All dependencies of this object.
		
		May be overridden to include conditional dependencies.
		
		Returns:
		  A list of `CheckpointableReference` objects indicating named
		  `Checkpointable` dependencies which should be saved along with this
		  object.
	**/
	public var _checkpoint_dependencies : Dynamic;
	/**
		A dictionary with deferred dependencies.
		
		Stores restorations for other Checkpointable objects on which this object
		may eventually depend. May be overridden by sub-classes (e.g. Optimizers use
		conditional dependencies based the current graph, and so need separate
		management of deferred dependencies too).
		
		Returns:
		  A dictionary mapping from local name to a list of _CheckpointPosition
		  objects.
	**/
	public var _deferred_dependencies : Dynamic;
	/**
		Returns a dictionary of values to checkpoint with this object.
		
		Keys in the returned dictionary are local to this object and in a separate
		namespace from dependencies. Values may either be `SaveableObject` factories
		or variables easily converted to `SaveableObject`s (as in `tf.train.Saver`'s
		`var_list` constructor argument).
		
		`SaveableObjects` have a name set, which Checkpointable needs to generate
		itself. So rather than returning `SaveableObjects` directly, this method
		should return a dictionary of callables which take `name` arguments and
		return `SaveableObjects` with that name.
		
		If this object may also be passed to the global-name-based `tf.train.Saver`,
		the returned callables should have a default value for their name argument
		(i.e. be callable with no arguments).
		
		Returned values must be saved only by this object; if any value may be
		shared, it should instead be a dependency. For example, variable objects
		save their own values with the key `VARIABLE_VALUE_KEY`, but objects which
		reference variables simply add a dependency.
		
		Returns:
		  The dictionary mapping attribute names to `SaveableObject` factories
		  described above. For example:
		  {VARIABLE_VALUE_KEY:
		   lambda name="global_name_for_this_object":
		   SaveableObject(name=name, ...)}
	**/
	public function _gather_saveables_for_checkpoint():Dynamic;
	/**
		Pop and load any deferred checkpoint restores into `checkpointable`.
		
		This method does not add a new dependency on `checkpointable`, but it does
		check if any outstanding/deferred dependencies have been queued waiting for
		this dependency to be added (matched based on `name`). If so,
		`checkpointable` and its dependencies are restored. The restorations are
		considered fulfilled and so are deleted.
		
		`_track_checkpointable` is more appropriate for adding a
		normal/unconditional dependency, and includes handling for deferred
		restorations. This method allows objects such as `Optimizer` to use the same
		restoration logic while managing conditional dependencies themselves, by
		overriding `_checkpoint_dependencies` and `_lookup_dependency` to change the
		object's dependencies based on the context it is saved/restored in (a single
		optimizer instance can have state associated with multiple graphs).
		
		Args:
		  name: The name of the dependency within this object (`self`), used to
		    match `checkpointable` with values saved in a checkpoint.
		  checkpointable: The Checkpointable object to restore (inheriting from
		    `CheckpointableBase`).
	**/
	public function _handle_deferred_dependencies(name:Dynamic, checkpointable:Dynamic):Dynamic;
	/**
		Look up a dependency by name.
		
		May be overridden to include conditional dependencies.
		
		Args:
		  name: The local name of the dependency.
		Returns:
		  A `Checkpointable` object, or `None` if no dependency by this name was
		  found.
	**/
	public function _lookup_dependency(name:Dynamic):Dynamic;
	/**
		Initialize dependency management.
		
		Not __init__, since most objects will forget to call it.
	**/
	public function _maybe_initialize_checkpointable():Dynamic;
	/**
		Restore the object's attributes from a name-based checkpoint.
	**/
	public function _name_based_attribute_restore(checkpoint:Dynamic):Dynamic;
	/**
		If automatic dependency tracking is enabled, ignores `value`.
	**/
	public function _no_dependency(value:Dynamic):Dynamic;
	/**
		Return a dependency's value for restore-on-create.
		
		Note the restoration is not deleted; if for some reason preload is called
		and then not assigned to the variable (for example because a custom getter
		overrides the initializer), the assignment will still happen once the
		variable is tracked (determined based on checkpoint.restore_uid).
		
		Args:
		  name: The object-local name of the dependency holding the variable's
		    value.
		  shape: The shape of the variable being loaded into.
		Returns:
		  An callable for use as a variable's initializer/initial_value, or None if
		  one should not be set (either because there was no variable with this name
		  in the checkpoint or because it needs more complex deserialization). Any
		  non-trivial deserialization will happen when the variable object is
		  tracked.
	**/
	public function _preload_simple_restoration(name:Dynamic, shape:Dynamic):Dynamic;
	/**
		Restore this object and its dependencies (may be deferred).
	**/
	public function _restore_from_checkpoint_position(checkpoint_position:Dynamic):Dynamic;
	/**
		Restore this object, and either queue its dependencies or defer them.
	**/
	public function _single_restoration_from_checkpoint_position(checkpoint_position:Dynamic, visit_queue:Dynamic):Dynamic;
	static public var _tf_api_names : Dynamic;
	static public var _tf_api_names_v1 : Dynamic;
	/**
		Declare a dependency on another `Checkpointable` object.
		
		Indicates that checkpoints for this object should include variables from
		`checkpointable`.
		
		Variables in a checkpoint are mapped to `Checkpointable`s based on the names
		provided when the checkpoint was written. To avoid breaking existing
		checkpoints when modifying a class, neither variable names nor dependency
		names (the names passed to `_track_checkpointable`) may change.
		
		Args:
		  checkpointable: A `Checkpointable` which this object depends on.
		  name: A local name for `checkpointable`, used for loading checkpoints into
		    the correct objects.
		  overwrite: Boolean, whether silently replacing dependencies is OK. Used
		    for __setattr__, where throwing an error on attribute reassignment would
		    be inappropriate.
		
		Returns:
		  `checkpointable`, for convenience when declaring a dependency and
		  assigning to a member variable in one statement.
		
		Raises:
		  TypeError: If `checkpointable` does not inherit from `Checkpointable`.
		  ValueError: If another object is already tracked by this name.
	**/
	public function _track_checkpointable(checkpointable:Dynamic, name:Dynamic, ?overwrite:Dynamic):Dynamic;
	/**
		Assigns a new value to the variable.
		
		This is essentially a shortcut for `assign(self, value)`.
		
		Args:
		  value: A `Tensor`. The new value for this variable.
		  use_locking: If `True`, use locking during the assignment.
		  name: The name of the operation to be created
		  read_value: if True, will return something which evaluates to the
		    new value of the variable; if False will return the assign op.
		
		Returns:
		  A `Tensor` that will hold the new value of this variable after
		  the assignment has completed.
	**/
	public function assign(value:Dynamic, ?use_locking:Dynamic, ?name:Dynamic, ?read_value:Dynamic):Dynamic;
	/**
		Adds a value to this variable.
		
		 This is essentially a shortcut for `assign_add(self, delta)`.
		
		Args:
		  delta: A `Tensor`. The value to add to this variable.
		  use_locking: If `True`, use locking during the operation.
		  name: The name of the operation to be created
		  read_value: if True, will return something which evaluates to the
		    new value of the variable; if False will return the assign op.
		
		Returns:
		  A `Tensor` that will hold the new value of this variable after
		  the addition has completed.
	**/
	public function assign_add(delta:Dynamic, ?use_locking:Dynamic, ?name:Dynamic, ?read_value:Dynamic):Dynamic;
	/**
		Subtracts a value from this variable.
		
		This is essentially a shortcut for `assign_sub(self, delta)`.
		
		Args:
		  delta: A `Tensor`. The value to subtract from this variable.
		  use_locking: If `True`, use locking during the operation.
		  name: The name of the operation to be created
		  read_value: if True, will return something which evaluates to the
		    new value of the variable; if False will return the assign op.
		
		Returns:
		  A `Tensor` that will hold the new value of this variable after
		  the subtraction has completed.
	**/
	public function assign_sub(delta:Dynamic, ?use_locking:Dynamic, ?name:Dynamic, ?read_value:Dynamic):Dynamic;
	/**
		Returns the constraint function associated with this variable.
		
		Returns:
		  The constraint function that was passed to the variable constructor.
		  Can be `None` if no constraint was passed.
	**/
	public var constraint : Dynamic;
	/**
		Increments this variable until it reaches `limit`.
		
		When that Op is run it tries to increment the variable by `1`. If
		incrementing the variable would bring it above `limit` then the Op raises
		the exception `OutOfRangeError`.
		
		If no error is raised, the Op outputs the value of the variable before
		the increment.
		
		This is essentially a shortcut for `count_up_to(self, limit)`.
		
		Args:
		  limit: value at which incrementing the variable raises an error.
		
		Returns:
		  A `Tensor` that will hold the variable value before the increment. If no
		  other Op modifies this variable, the values produced will all be
		  distinct.
	**/
	public function count_up_to(limit:Dynamic):Dynamic;
	/**
		The device of this variable.
	**/
	public var device : Dynamic;
	/**
		The `DType` of this variable.
	**/
	public var dtype : Dynamic;
	/**
		In a session, computes and returns the value of this variable.
		
		This is not a graph construction method, it does not add ops to the graph.
		
		This convenience method requires a session where the graph
		containing this variable has been launched. If no session is
		passed, the default session is used.  See `tf.Session` for more
		information on launching a graph and on sessions.
		
		```python
		v = tf.Variable([1, 2])
		init = tf.global_variables_initializer()
		
		with tf.Session() as sess:
		    sess.run(init)
		    # Usage passing the session explicitly.
		    print(v.eval(sess))
		    # Usage with the default session.  The 'with' block
		    # above makes 'sess' the default session.
		    print(v.eval())
		```
		
		Args:
		  session: The session to use to evaluate this variable. If
		    none, the default session is used.
		
		Returns:
		  A numpy `ndarray` with a copy of the value of this variable.
	**/
	public function eval(?session:Dynamic):Dynamic;
	/**
		Returns a `Variable` object created from `variable_def`.
	**/
	static public function from_proto(variable_def:Dynamic, ?import_scope:Dynamic):Dynamic;
	/**
		Alias of Variable.shape.
	**/
	public function get_shape():Dynamic;
	/**
		The `Graph` of this variable.
	**/
	public var graph : Dynamic;
	/**
		Returns the Tensor used as the initial value for the variable.
		
		Note that this is different from `initialized_value()` which runs
		the op that initializes the variable before returning its value.
		This method returns the tensor that is used by the op that initializes
		the variable.
		
		Returns:
		  A `Tensor`.
	**/
	public var initial_value : Dynamic;
	/**
		Returns the value of the initialized variable.
		
		You should use this instead of the variable itself to initialize another
		variable with a value that depends on the value of this variable.
		
		```python
		# Initialize 'v' with a random tensor.
		v = tf.Variable(tf.truncated_normal([10, 40]))
		# Use `initialized_value` to guarantee that `v` has been
		# initialized before its value is used to initialize `w`.
		# The random values are picked only once.
		w = tf.Variable(v.initialized_value() * 2.0)
		```
		
		Returns:
		  A `Tensor` holding the value of this variable after its initializer
		  has run.
	**/
	public function initialized_value():Dynamic;
	/**
		The initializer operation for this variable.
	**/
	public var initializer : Dynamic;
	/**
		Load new value into this variable.
		
		Writes new value to variable's memory. Doesn't add ops to the graph.
		
		This convenience method requires a session where the graph
		containing this variable has been launched. If no session is
		passed, the default session is used.  See `tf.Session` for more
		information on launching a graph and on sessions.
		
		```python
		v = tf.Variable([1, 2])
		init = tf.global_variables_initializer()
		
		with tf.Session() as sess:
		    sess.run(init)
		    # Usage passing the session explicitly.
		    v.load([2, 3], sess)
		    print(v.eval(sess)) # prints [2 3]
		    # Usage with the default session.  The 'with' block
		    # above makes 'sess' the default session.
		    v.load([3, 4], sess)
		    print(v.eval()) # prints [3 4]
		```
		
		Args:
		    value: New variable value
		    session: The session to use to evaluate this variable. If
		      none, the default session is used.
		
		Raises:
		    ValueError: Session is not passed and no default session
	**/
	public function load(value:Dynamic, ?session:Dynamic):Dynamic;
	/**
		The name of this variable.
	**/
	public var name : Dynamic;
	/**
		The `Operation` of this variable.
	**/
	public var op : Dynamic;
	/**
		Returns the value of this variable, read in the current context.
		
		Can be different from value() if it's on another device, with control
		dependencies, etc.
		
		Returns:
		  A `Tensor` containing the value of the variable.
	**/
	public function read_value():Dynamic;
	/**
		Adds `IndexedSlices` to this variable.
		
		Args:
		  sparse_delta: `IndexedSlices` to be assigned to this variable.
		  use_locking: If `True`, use locking during the operation.
		  name: the name of the operation.
		
		Returns:
		  A `Tensor` that will hold the new value of this variable after
		  the scattered subtraction has completed.
		
		Raises:
		  ValueError: if `sparse_delta` is not an `IndexedSlices`.
	**/
	public function scatter_add(sparse_delta:Dynamic, ?use_locking:Dynamic, ?name:Dynamic):Dynamic;
	/**
		Applies sparse addition to individual values or slices in a Variable.
		
		`ref` is a `Tensor` with rank `P` and `indices` is a `Tensor` of rank `Q`.
		
		`indices` must be integer tensor, containing indices into `ref`.
		It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 < K <= P`.
		
		The innermost dimension of `indices` (with length `K`) corresponds to
		indices into elements (if `K = P`) or slices (if `K < P`) along the `K`th
		dimension of `ref`.
		
		`updates` is `Tensor` of rank `Q-1+P-K` with shape:
		
		```
		[d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]].
		```
		
		For example, say we want to add 4 scattered elements to a rank-1 tensor to
		8 elements. In Python, that update would look like this:
		
		```python
		    ref = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])
		    indices = tf.constant([[4], [3], [1] ,[7]])
		    updates = tf.constant([9, 10, 11, 12])
		    add = ref.scatter_nd_add(indices, updates)
		    with tf.Session() as sess:
		      print sess.run(add)
		```
		
		The resulting update to ref would look like this:
		
		    [1, 13, 3, 14, 14, 6, 7, 20]
		
		See `tf.scatter_nd` for more details about how to make updates to
		slices.
		
		Args:
		  indices: The indices to be used in the operation.
		  updates: The values to be used in the operation.
		  name: the name of the operation.
		
		Returns:
		  A `Tensor` that will hold the new value of this variable after
		  the scattered subtraction has completed.
		
		Raises:
		  ValueError: if `sparse_delta` is not an `IndexedSlices`.
	**/
	public function scatter_nd_add(indices:Dynamic, updates:Dynamic, ?name:Dynamic):Dynamic;
	/**
		Applies sparse subtraction to individual values or slices in a Variable.
		
		`ref` is a `Tensor` with rank `P` and `indices` is a `Tensor` of rank `Q`.
		
		`indices` must be integer tensor, containing indices into `ref`.
		It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 < K <= P`.
		
		The innermost dimension of `indices` (with length `K`) corresponds to
		indices into elements (if `K = P`) or slices (if `K < P`) along the `K`th
		dimension of `ref`.
		
		`updates` is `Tensor` of rank `Q-1+P-K` with shape:
		
		```
		[d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]].
		```
		
		For example, say we want to add 4 scattered elements to a rank-1 tensor to
		8 elements. In Python, that update would look like this:
		
		```python
		    ref = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])
		    indices = tf.constant([[4], [3], [1] ,[7]])
		    updates = tf.constant([9, 10, 11, 12])
		    op = ref.scatter_nd_sub(indices, updates)
		    with tf.Session() as sess:
		      print sess.run(op)
		```
		
		The resulting update to ref would look like this:
		
		    [1, -9, 3, -6, -6, 6, 7, -4]
		
		See `tf.scatter_nd` for more details about how to make updates to
		slices.
		
		Args:
		  indices: The indices to be used in the operation.
		  updates: The values to be used in the operation.
		  name: the name of the operation.
		
		Returns:
		  A `Tensor` that will hold the new value of this variable after
		  the scattered subtraction has completed.
		
		Raises:
		  ValueError: if `sparse_delta` is not an `IndexedSlices`.
	**/
	public function scatter_nd_sub(indices:Dynamic, updates:Dynamic, ?name:Dynamic):Dynamic;
	/**
		Applies sparse assignment to individual values or slices in a Variable.
		
		`ref` is a `Tensor` with rank `P` and `indices` is a `Tensor` of rank `Q`.
		
		`indices` must be integer tensor, containing indices into `ref`.
		It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 < K <= P`.
		
		The innermost dimension of `indices` (with length `K`) corresponds to
		indices into elements (if `K = P`) or slices (if `K < P`) along the `K`th
		dimension of `ref`.
		
		`updates` is `Tensor` of rank `Q-1+P-K` with shape:
		
		```
		[d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]].
		```
		
		For example, say we want to add 4 scattered elements to a rank-1 tensor to
		8 elements. In Python, that update would look like this:
		
		```python
		    ref = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])
		    indices = tf.constant([[4], [3], [1] ,[7]])
		    updates = tf.constant([9, 10, 11, 12])
		    op = ref.scatter_nd_assign(indices, updates)
		    with tf.Session() as sess:
		      print sess.run(op)
		```
		
		The resulting update to ref would look like this:
		
		    [1, 11, 3, 10, 9, 6, 7, 12]
		
		See `tf.scatter_nd` for more details about how to make updates to
		slices.
		
		Args:
		  indices: The indices to be used in the operation.
		  updates: The values to be used in the operation.
		  name: the name of the operation.
		
		Returns:
		  A `Tensor` that will hold the new value of this variable after
		  the scattered subtraction has completed.
		
		Raises:
		  ValueError: if `sparse_delta` is not an `IndexedSlices`.
	**/
	public function scatter_nd_update(indices:Dynamic, updates:Dynamic, ?name:Dynamic):Dynamic;
	/**
		Subtracts `IndexedSlices` from this variable.
		
		Args:
		  sparse_delta: `IndexedSlices` to be subtracted from this variable.
		  use_locking: If `True`, use locking during the operation.
		  name: the name of the operation.
		
		Returns:
		  A `Tensor` that will hold the new value of this variable after
		  the scattered subtraction has completed.
		
		Raises:
		  ValueError: if `sparse_delta` is not an `IndexedSlices`.
	**/
	public function scatter_sub(sparse_delta:Dynamic, ?use_locking:Dynamic, ?name:Dynamic):Dynamic;
	/**
		Assigns `IndexedSlices` to this variable.
		
		Args:
		  sparse_delta: `IndexedSlices` to be assigned to this variable.
		  use_locking: If `True`, use locking during the operation.
		  name: the name of the operation.
		
		Returns:
		  A `Tensor` that will hold the new value of this variable after
		  the scattered subtraction has completed.
		
		Raises:
		  ValueError: if `sparse_delta` is not an `IndexedSlices`.
	**/
	public function scatter_update(sparse_delta:Dynamic, ?use_locking:Dynamic, ?name:Dynamic):Dynamic;
	/**
		Overrides the shape for this variable.
		
		Args:
		  shape: the `TensorShape` representing the overridden shape.
	**/
	public function set_shape(shape:Dynamic):Dynamic;
	/**
		The `TensorShape` of this variable.
		
		Returns:
		  A `TensorShape`.
	**/
	public var shape : Dynamic;
	/**
		Converts a `Variable` to a `VariableDef` protocol buffer.
		
		Args:
		  export_scope: Optional `string`. Name scope to remove.
		
		Returns:
		  A `VariableDef` protocol buffer, or `None` if the `Variable` is not
		  in the specified name scope.
	**/
	public function to_proto(?export_scope:Dynamic):Dynamic;
	public var trainable : Dynamic;
	/**
		Returns the last snapshot of this variable.
		
		You usually do not need to call this method as all ops that need the value
		of the variable call it automatically through a `convert_to_tensor()` call.
		
		Returns a `Tensor` which holds the value of the variable.  You can not
		assign a new value to this tensor as it is not a reference to the variable.
		
		To avoid copies, if the consumer of the returned value is on the same device
		as the variable, this actually returns the live value of the variable, not
		a copy.  Updates to the variable are seen by the consumer.  If the consumer
		is on a different device it will get a copy of the variable.
		
		Returns:
		  A `Tensor` containing the value of the variable.
	**/
	public function value():Dynamic;
}